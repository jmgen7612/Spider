#!/usr/bin/env python3# -*- coding: utf-8 -*-#@file   :HtmlParser.py#@time   :2018/5/26 2:01#@Author :jmgen#@Version:1.0#@Desc   :#coding:utf-8import urllib.parseimport urllib.requestfrom bs4 import BeautifulSoupimport requestsimport datetimefrom LagouSpider.wayTwo.HtmlDownloader import HtmlDownloaderclass HtmlParser(object):    def parser(self,page_url,html_cont):        '''        用于解析网页内容抽取URL和数据        :param page_url: 下载页面的URL        :param html_cont: 下载的网页内容        :return:返回URL和数据        '''        if page_url is None or html_cont is None:            return        soup = BeautifulSoup(html_cont,'html.parser',from_encoding='utf-8')        new_data = self._get_new_data(page_url,soup)        # print(new_data)        return new_data    def get_datas(self,request_url,page_num,keywords,baseurl,params,headers):        datas = self._parser_response(request_url, page_num,keywords, baseurl, params, headers)        return datas    def _parser_response(self,request_url,page_num,keywords,baseurl,params,headers):        '''        解析response的数据        :param request_url: 请求链接        :param data 请求数据        :param baseurl 基础链接        :param params 筛选关键字        :param headers 头文件        :return: 返回response的result        '''        data = urllib.parse.urlencode([            ('pn', page_num),            ('kd', keywords)        ])        page = requests.post(url=request_url, headers=headers, data=data, params=params)        page.encoding = 'utf-8'        result = page.json()        jobs = result.get('content').get('positionResult').get('result')        datas = self._parser_json_datas(baseurl, jobs,headers)        return datas        # try:        #        # except Exception:        #     print(Exception, request_url)        #     return None    def _parser_json_datas(self, baseurl,jobs,headers):        datas=list()        for job in jobs:            data=self._parser_json_data(baseurl,job,headers)            datas.append(data)        return datas    def _parser_json_data(self, baseurl,job,headers):        try:            positionId = job['positionId']  # 职位ID            positionName = job['positionName']  # 职位名称            companyFullName = job['companyFullName']  # 公司全名            companyId = job['companyId']  # 公司ID            workYear = job['workYear']  # 工作年限            salary = job['salary']  # 工资            education = job['education']  # 学历            city = job['city']  # 城市            district = str(job['district'])  # 区            businessZones = ','.join(job['businessZones'])  # 范围            financeStage = job['financeStage']  # 融资阶段            industryField = job['industryField']  # 工业领域            companySize = job['companySize']  # 公司人数            positionAdvantage = job['positionAdvantage']  # 公司优势            companyLabelList = ','.join(job['companyLabelList'])  # 公司标签            firstType = job['firstType']  # 职位类型            secondType = job['secondType']  # 职位细类            createTime = job['createTime']  # 创建时间            lastLogin = str(datetime.datetime.utcfromtimestamp(job['lastLogin'] / 1000).strftime("%Y-%m-%d %H:%M:%S"))            resumeProcessRate = job['resumeProcessRate']  # 简历处理率            resumeProcessDay = job['resumeProcessDay']  # 简历处理用时            new_full_url = baseurl + str(positionId) + ".html"            self.downloader = HtmlDownloader()            html_cont = self.downloader.download(new_full_url, headers)            pagedata = self.parser(new_full_url, html_cont)            # 将提取其中的内容进行返回            return (positionId, positionName, companyFullName, companyId, workYear,                    salary, education, city, district, businessZones, financeStage,                    industryField, companySize, positionAdvantage, companyLabelList,                    firstType, secondType, createTime, lastLogin, resumeProcessRate,                    resumeProcessDay, pagedata[0], pagedata[1], pagedata[2], pagedata[3])        except Exception:            print(Exception, job)            return None    def _get_new_data(self,page_url,soup):        '''        抽取有效数据        :param page_url:下载页面的URL        :param soup:        :return:返回有效数据        '''        try:            joblabel = soup.find('dd', class_='job_request').find('ul').get_text().replace('\n',',')[1:-1]            jobdescription = soup.find('dd', class_='job_bt').find('div').get_text().replace('\n',',')[1:-1].replace('\xa0','')            jobaddress = soup.find('dd', class_='job-address clearfix').find('div').get_text().replace('\n','').replace(' ','')[:-4]            return (joblabel, jobdescription, jobaddress, page_url)        except Exception:            print(Exception, page_url)            return None