#!/usr/bin/env python3# -*- coding: utf-8 -*-#@file   :HtmlParser.py#@time   :2018/5/26 2:01#@Author :jmgen#@Version:1.0#@Desc   :#coding:utf-8import urllib.parseimport urllib.requestfrom bs4 import BeautifulSoupimport requestsimport datetimeclass HtmlParser(object):    def parser(self,page_url,html_cont):        '''        用于解析网页内容抽取URL和数据        :param page_url: 下载页面的URL        :param html_cont: 下载的网页内容        :return:返回URL和数据        '''        if page_url is None or html_cont is None:            return        soup = BeautifulSoup(html_cont,'html.parser',from_encoding='utf-8')        new_data = self._get_new_data(page_url,soup)        return new_data    def get_url(self,request_url,page_num,keywords,baseurl,params,headers):        urls,datas=self._parser_response(request_url,page_num,keywords,baseurl,params,headers)        return urls    def get_datas(self,request_url,page_num,keywords,baseurl,params,headers):        urls, datas = self._parser_response(request_url, page_num,keywords, baseurl, params, headers)        return datas    def _parser_response(self,request_url,page_num,keywords,baseurl,params,headers):        '''        解析response的数据        :param request_url: 请求链接        :param data 请求数据        :param baseurl 基础链接        :param params 筛选关键字        :param headers 头文件        :return: 返回response的result        '''        data = urllib.parse.urlencode([            ('pn', page_num),            ('kd', keywords)        ])        try:            page = requests.post(url=request_url, headers=headers, data=data, params=params)            page.encoding = 'utf-8'            result = page.json()            jobs = result.get('content').get('positionResult').get('result')            urls = self._parser_json_urls(baseurl, jobs)            datas = self._parser_json_datas(jobs)            return urls, datas        except Exception:            print(Exception, request_url)            return None    def _parser_json_urls(self, baseurl,jobs):        new_urls = set()        for job in jobs:            positionId = job['positionId']  # 主页ID            new_full_url = baseurl + str(positionId) + ".html"            new_urls.add(new_full_url)        return new_urls    def _parser_json_datas(self, jobs):        datas=list()        for job in jobs:            data=self._parser_json_data(job)            print(data)            datas.append(data)        return datas    def _parser_json_data(self, job):        try:            positionId = job['positionId']  # 职位ID            positionName = job['positionName']  # 职位名称            companyFullName = job['companyFullName']  # 公司全名            companyId = job['companyId']  # 公司ID            workYear = job['workYear']  # 工作年限            salary = job['salary']  # 工资            education = job['education']  # 学历            city = job['city']  # 城市            district = str(job['district'])  # 区            businessZones = ','.join(job['businessZones'])  # 范围            financeStage = job['financeStage']  # 融资阶段            industryField = job['industryField']  # 工业领域            companySize = job['companySize']  # 公司人数            positionAdvantage = job['positionAdvantage']  # 公司优势            companyLabelList = ','.join(job['companyLabelList'])  # 公司标签            firstType = job['firstType']  # 职位类型            secondType = job['secondType'] # 职位细类            createTime = job['createTime']  # 创建时间            lastLogin = str(datetime.datetime.utcfromtimestamp(job['lastLogin']/1000).strftime("%Y-%m-%d %H:%M:%S"))            resumeProcessRate = job['resumeProcessRate']  # 简历处理率            resumeProcessDay = job['resumeProcessDay']  # 简历处理用时            # 将提取其中的内容进行返回            return (positionId, positionName, companyFullName, companyId, workYear,                    salary, education, city, district, businessZones,financeStage,                    industryField, companySize,positionAdvantage,companyLabelList,                    firstType,secondType,createTime, lastLogin, resumeProcessRate,                    resumeProcessDay)        except Exception:            print(Exception, job)            return None    def _get_new_data(self,page_url,soup):        '''        抽取有效数据        :param page_url:下载页面的URL        :param soup:        :return:返回有效数据        '''        try:            positionId = soup.find('a', class_='send-CV-btn s-send-btn fr').get('data-position-id')            joblabel = soup.find('dd', class_='job_request').find('ul').get_text().replace('\n',',')[1:-1]            jobdescription = soup.find('dd', class_='job_bt').find('div').get_text().replace('\n',',')[1:-1].replace('\xa0','')            jobaddress = soup.find('dd', class_='job-address clearfix').find('div').get_text().replace('\n','').replace(' ','')[:-4]            return (positionId, joblabel, jobdescription, jobaddress, page_url)        except Exception:            print(Exception, page_url)            return None